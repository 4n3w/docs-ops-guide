---
title: Key Metrics for Monitoring a Pivotal Cloud Foundry Deployment
---

The Pivotal Cloud Ops team monitors the health of its Cloud Foundry deployments using a customized Datadog dashboard. This topic describes each of the key metrics as they are rendered in the custom dashboard, and why they are useful for monitoring the health of a Cloud Foundry deployment.

Cloud Ops' practices are tailored to the specific details of the Cloud Foundry deployments they operate.  Therefore, the descriptions here are meant to be informative examples rather than general prescriptions. Pivotal recommends that operators experiment with different combinations of metrics and alerts appropriate to their specific requirements.

The Cloud Ops team's custom configuration of Datadog's dashboards, alerts and screenboards can be found in the [Datadog Config repository](https://github.com/pivotal-cf-experimental/datadog-config-oss).

<%= image_tag 'metrics/dashboard.png' %>

## <a id='bosh-health'></a>BOSH Health Monitor

<%= image_tag 'metrics/bosh_health_combined.png'%>

<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>Health, broken down by component. Each row displays the average percentage of healthy instances for the relevant component over the last 5 minutes and 24 hours.<br>For example, suppose your Router has ten instances. If one instance is not healthy, the stoplight turns red and shows 90%. <br><br>We monitor health for the following components:<br><br>
		<ul>
			<li>
				Backend:
				<ul>
					<li>NATS</li>
					<li>Doppler</li>
					<li>Stats</li>
					<li>HM9000</li>
					<li>BOSH</li>
					<li>NAT Box</li>
					<li>ETCD</li>
				</ul>
			</li>
			<li>
				Frontend:
				<ul>
					<li>Router</li>
					<li>DEA</li>
					<li>API</li>
					<li>UAA</li>
				</ul>
			</li>
		<ul>
	</td>
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>
		To ensure all VMs are functioning properly.
	</td>
</tr>
<tr>
	<th>System metric</th>
	<td><code>bosh.healthmonitor.system.healthy</code></td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>None</td>
</tr>
<tr>
	<th>Notes</th>
	<td>
		Alerts generated from this metric go to a buffer queue in our alerting system, Pagerduty. Because BOSH restores systems quickly when they go down, we wait two minutes before forwarding unresolved alerts to our operators.</td>
</tr>
</table>

## <a id='req-per-sec'></a>Requests per second

<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>
		Requests per second for each of the following components:
		<ul>
			<li>Router</li>
			<li>DEA</li>
			<li>API</li>
			<li>UAA</li>
		</ul>
	</td>
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>To track the flow of traffic through the components in the system.</td>
</tr>
<tr>
	<th>System metric</th>
	<td><code>cf.collector.router.requests(component: app/cloudcontroller/uaa)</code></td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>None</td>
</tr>
<tr>
	<th>Notes</th>
	<td>None</td>
</tr>
</table>

## <a id='nats-traffic-delta'></a>NATS Traffic Delta

<% image_tag 'metrics/nats_delta.png' %>

<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>
		Delta of average NATS traffic over the last hour.
		<br><br>
		The displayed metric is the difference between the average NATS traffic over the last 30 minutes and the average NATS traffic over the interval from 90 to 60 minutes prior.
	</td>
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>To detect significant drops in NATS traffic. A sudden drop might indicate a problem with the health of the NATS VMs.</td>
</tr>
<tr>
	<th>System metric</th>
	<td><code>aws.ec2.network_in</code></td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>None</td>
</tr>
<tr>
	<th>Notes</th>
	<td>None</td>
</tr>
</table>

## <a id='etcd-uptime'></a>ETCD Leader Uptime

<% image_tag 'metrics/etcd_uptime.png' %>


<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>
		Time since the ETCD leader last was down.
	</td>
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>When the ETCD leader goes down, it usually indicates a push failure.</td>
</tr>
<tr>
	<th>System metric</th>
	<td><code>cloudops_tools.etcd_leader_health</code></td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>None</td>
</tr>
<tr>
	<th>Notes</th>
	<td>The `cloudops_tools` metrics are generated by an internal app developed by the Pivotal Cloud Ops team.</td>
</tr>
</table>


## <a id='ssh-attempts'></a>SSH Attempts

<% image_tag 'metrics/ssh_attempts.png' %>


<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>Count of iptables <code>first packet</code> to port 22 anywhere on the internet.</td>
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>A spike in SSH attempts is a good indicator of SSH-cracker attacks.</td>
</tr>
<tr>
	<th>System metric</th>
	<td><code>cloudops_tools.ssh-abuse-monitor</code></td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>None</td>
</tr>
<tr>
	<th>Notes</th>
	<td>
		<ul>
			<li>
				This metric is generated by a private internal app called ssh-abuse-monitor maintained by Cloud Ops. DEAs send their iptables logs to Logsearch, and ssh-abuse-monitor polls logsearch and pushes a 
			</li>
			<li>
				The `cloudops_tools` metrics are generated by an internal app developed by the Pivotal Cloud Ops team.
			</li>
		</ul>
	</td>
</tr>
</table>


# <a id='router-column'></a>The Router Status Column

<%= image_tag 'metrics/router-column.png' %>

## <a id='app-instance-count'></a>App Instance Count


<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>Count of running app instances</td>
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>Big swings in app count can indicate malicious user behavior or CF component issues, either of which are important for Cloud Ops to address quickly.</td>
</tr>
<tr>
	<th>System metric</th>
	<td>
		<code>avg:cf.collector.HM9000.HM9000.NumberOfAppsWithAllInstancesReporting</code>
	</td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>running app number change rate</td>
</tr>
<tr>
	<th>Notes</th>
	<td>If this spikes up, you need to pay attention to the Runner/Stager metrics.

	</td>
</tr>
</table>

## <a id='total-routes'></a>Total Routes


<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>Count of routes held by the GoRouter.</td>
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>The count on all routers should be the same. If this differs between routers, it usually indicates a NATS problem.
	</td>
</tr>
<tr>
	<th>System metric</th>
	<td><code>cf.collector.router.total_routes</code></td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>prod CF: Number of routes in the router's routing table is too low</td>
</tr>
<tr>
	<th>Notes</th>
	<td
		<ul>
			<li>
				We show the delta over the last N min so that an operator can see an absolute change rather than counting on the shape of the graph, which can be misleading.
			</li>
			<li>
				The router are “the” way into all CF systems (both CF components and customer apps). If they go down (or don’t have routes) the system is down. It’s a keystone component and understanding what it’s metrics tell us is important. This is a metric with a “cliff”: nominal changes are < 20, if we are doing a marketing event we might see ~200. Those are okay. Outside of that we really only see full failure ( minus 4000+ routes). There is nothing in between.
			</li>
		</ul>
	</td>
</tr>
</table>

## <a id='router-dial-errors'></a>Router Dial Errors


<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>5xx from the routers to backend CF components and Apps.
There are separate measures for CF Components and User pushed apps.</td>
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>This tells us is we have failures connecting to components.</td>
</tr>
<tr>
	<th>System metric</th>
	<td><code>avg:cloudops_tools.app_instance_monitor.router.dial.errors{domain:run.pivotal.io} / avg:cloudops_tools.app_instance_monitor.router.dial.errors{cf_component:false}</code></td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>
		<ul>
			<li>No data for router dial errors</li>
			<li>Router dial errors for console.run.pivotal.io</li>
			<li>Too many router dial errors for cf components</li>
		</ul>
	</td>
</tr>
<tr>
	<th>Notes</th>
	<td>
		<ul>
			<li>
				Any dial errors to CC, UAA, Dopplers and any other BOSH-deployed CF component must be investigated. Same for admin domain apps (<code>*run.pivotal.io</code> in our case). We expect dial errors from our large population of customer app (4000+). People push bad apps, or are running dev iterations, etc and will have 502s. We don’t alert on them, we observe that 5xx in the 500/10 min range are normal. If we saw this number to jump to 1000+/10 min, we’d investigate.
			</li>
			<li>
				The `cloudops_tools` metrics are generated by an internal app developed by the Pivotal Cloud Ops team.
			</li>
		</ul>
	</td>
</tr>
</table>


## <a id='rotuer-cpu'></a>Router CPU


<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>OS-level CPU usage.</td>
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>Routers are multi-threaded and like a lot of CPU, if they are using too much CPU we will scale them via BOSH.</td>
</tr>
<tr>
	<th>System metric</th>
	<td><code>bosh.healthmonitor.system.cpu.user{deployment:cf-cfapps-io2,job:<router_job>}</code></td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>None</td>
</tr>
<tr>
	<th>Notes</th>
	<td>Our rule of thumb is: Scaling is easy, if in doubt, add some routers.</td>
</tr>
</table>



## <a id="aws-events"></a>AWS Events


<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>Feed from ‘aws ec2 events’ </td>
	<!-- check to make sure this is right, engineer doc had this flagged as unsure -->
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>Tells us about VM, RDS and other things critical to know from our IaaS.</td>
</tr>
<tr>
	<th>System metric</th>
	<td>N/A</td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>None</td>
</tr>
<tr>
	<th>Notes</th>
	<td>Only applies to CloudFoundry deployments on AWS.</td>
</tr>
</table>

#  <a id='dea-status-column'></a> The DEA Status Column

<%= image_tag 'metrics/dea_status_column.png' %>


## <a id='reservable-stagers'></a>Reservable Stagers

<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>How many 1GB apps can be staged.</td>
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>Better capacity planning. If the number is too low users will not be able to stage more apps or even scale their apps.</td>
</tr>
<tr>
	<th>System metric</th>
	<td><code>sum:cf.collector.reservable_stagers</code></td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>Alerts when there aren’t enough reservable stagers.</td>
</tr>
<tr>
	<th>Notes</th>
	<td>Used to calculate DEA runners needed.</td>
</tr>
</table>


## <a id='mem-free-bytes'></a>Mem Free Bytes

<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>Total free memory on the DEAs</td>
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>Better capacity planning. If the number is too low, users will not be able to stage more apps or scale staged apps.</td>
</tr>
<tr>
	<th>System metric</th>
	<td><code>cf.collector.mem_free_bytes{job:dea}</code></td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>None</td>
</tr>
<tr>
	<th>Notes</th>
	<td>Used to calculate DEA runners needed.</td>
</tr>
</table>

## <a id='hours-since-last-start'></a>Hours since last DEA start


<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>Number of hours since the last DEA was started.</td>
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>If DEAs are restarting frequently this can indicate a bug in the DEA. This can directly impact customer-facing performance as it can cause apps to constantly be restaged.</td>
</tr>
<tr>
	<th>System metric</th>
	<td><code>(min:cf.collector.uptime_in_seconds{job:dea}) / 3600</code></td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>None</td>
</tr>
<tr>
	<th>Notes</th>
	<td>None</td>
</tr>
</table>

## <a id='over-4gb-ram'></a>Percent of DEAs with more than 4 GB free RAM


<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>What percentage of existing DEAs are available to stage 4 GB apps.</td>
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>To make sure a user can push a 4 GB app.</td>
</tr>
<tr>
	<th>System metric</th>
	<td><code>avg:cloudops_tools.dea_monitor.reservable_stagers_aggregate * 100</code></td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>None</td>
</tr>
<tr>
	<th>Notes</th>
	<td>None</td>
</tr>
</table>

## <a id='app-stage-fail'></a>Application Staging Failures


<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>How many app staging requests failed on the runner.</td>
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>To ensure that users are not facing issues staging apps.</td>
</tr>
<tr>
	<th>System metric</th>
	<td><code>avg:cloudops_tools.failed_stages.counts_per_5min</code></td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>None</td>
</tr>
<tr>
	<th>Notes</th>
	<td>None</td>
</tr>
</table>

## <a id='dea-runners-needed'></a>DEA runners needed


<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>If there are any DEA runners needed.</td>
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>To tell us how many more runners we might need to add.</td>
</tr>
<tr>
	<th>System metric</th>
	<td><code>avg:cloudops_tools.dea_monitor.additional_runners_needed</code></td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>None</td>
</tr>
<tr>
	<th>Notes</th>
	<td>The cloudops_tools metrics are generated by an internal app developed by the Pivotal Cloud Ops team.</td>
</tr>
</table>

## <a id='stage-per-hour'></a>Stages per hour

<table border='1' class='nice'>
<tr>
	<th>What we monitor</th>
	<td>How many apps are being staged every hour.</td>
</tr>
<tr>
	<th>Why we monitor it</th>
	<td>This metric is useful for statistical purposes and debugging issues with the system. If there are no apps being staged for 15 minutes, we might want to look at the system and see if anything is wrong.</td>
</tr>
<tr>
	<th>System metric</th>
	<td><code>avg:cloudops_tools.stages.counts_per_5min.rollup(sum,3600)</code></td>
</tr>
<tr>
	<th>Alerts triggered</th>
	<td>None</td>
</tr>
<tr>
	<th>Notes</th>
	<td>None</td>
</tr>
</table>





