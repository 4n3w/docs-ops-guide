---
title: Monitoring a Pivotal Cloud Foundry Deployment
owner: Cloud Ops
---

<strong><%= modified_date %></strong>

This topic describes how to set up Pivotal Cloud Foundry (PCF) with third-party monitoring platforms to continuously monitor system metrics and trigger health alerts.

To perform a manual, one-time check of current PCF system status from Ops Manager, see [Monitoring Virtual Machines in Pivotal Cloud Foundry](../customizing/monitoring.html).

Pivotal recommends that operators experiment with different combinations of metrics and alerts appropriate to their specific requirements.

As a prerequisite to PCF monitoring, you need an account with a monitoring platform, such as [OpenTDSB](http://opentsdb.net/) or similar platforms. This topic uses examples from Datadog, and the [Datadog Config repository](https://github.com/pivotal-cf-experimental/datadog-config-oss/blob/master/cloud-ops-example-doc.md) shows how the Pivotal Cloud Ops team monitors the health of its Cloud Foundry deployments using a customized Datadog dashboard.

<p class='note'><strong>Note</strong>: Pivotal does not officially support Datadog.</p>

## <a id='overview'></a>Overview

To set up PCF monitoring, you then configure PCF and your monitoring platform as follows:

* In PCF:
	- Install a [nozzle](../loggregator/architecture.html#nozzles) that extracts BOSH and CF metrics from the Firehose and sends them to the monitoring platform.
	- Create a mechanism that periodically generates smoke tests and other custom metrics. (Recommended for production systems)

* In your monitoring platform:
  - Customize a dashboard that lets you check and diagnose system health.
  - Create alerts that generate communications regarding attention-worthy conditions.

## <a id='sources'></a>Metrics Sources

In PCF, the Loggregator [Firehose](../loggregator/architecture.html#firehose) endpoint streams metrics and logs aggregated from all Elastic Runtime component VMs, both system components and hosts. These metrics come from two sources: the BOSH health monitor and Cloud Foundry components. In addition, you can deploy an app to run regular smoke tests and generate test result metrics other custom metrics that are important for monitoring.

### <a id='bosh'></a> BOSH Health Monitor

The BOSH layer that underlies PCF generates `healthmonitor` metrics for all virtual machines (VMs) in the deployment. For monitoring system health, the most important of these are:

* `bosh.healthmonitor.system.cpu`: CPU usage, percent of total available on VM
* `bosh.healthmonitor.system.mem`: Memory usage, percent of total available on VM
* `bosh.healthmonitor.system.disk`: Disk usage, percent of total available on VM
* `bosh.healthmonitor.system.healthy`: `1` if VM is healthy, `0` otherwise

### <a id='cf'></a>Cloud Foundry Components

Cloud Foundry component VMs for executive control, hosting, routing, traffic control, authentication, and other internal functions generate metrics. See [Cloud Foundry Component Metrics](../loggregator/all_metrics.html) for a detailed listing of metrics generated by Cloud Foundry.

Component metrics that are often useful to monitor include:

* `auctioneer.AuctioneerFetchStatesDuration`
* `auctioneer.AuctioneerLRPAuctionsFailed`
* `bbs.Domain.cf_apps`
* `bbs.CrashedActualLRPs`
* `bbs.LRPsMissing`
* `bbs.ConvergenceLRPDuration`
* `bbs.RequestLatency`
* `DopplerServer.listeners.receivedEnvelopes`
* `DopplerServer.TruncatingBuffer.totalDroppedMessages`
* `gorouter.total_routes`
* `gorouter.ms_since_last_registry_update`
* `MetronAgent.dropsondeMarshaller.sentEnvelopes`
* `nsync_bulker.DesiredLRPSyncDuration`
* `rep.CapacityRemainingMemory`
* `rep.CapacityTotalMemory`
* `rep.RepBulkSyncDuration`
* `route_emitter.RouteEmitterSyncDuration`

CF components specific to your IaaS also generate key metrics for health monitoring.

### <a id='smoke-tests'></a> Smoke Tests and Other Custom Metrics

PCF includes [smoke tests](https://github.com/pivotal-cloudops/cf-smoke-tests/tree/Dockerfile), which are functional unit and integration tests on all major system components. By default, whenever an operator upgrades to a new version of Elastic Runtime, these smoke tests run as a post-deploy errand.

Production systems typically also have an app that runs smoke tests periodically, for example every five minutes, and generate pass/fail metrics from the results.

Smoke tests are the most valuable metrics overall for monitoring a PCF deployment, but operators can also generate other custom metrics based on multi-component tests. An example is average outbound latency between components.

## <a id='send'></a>Send Metrics to a Monitoring Platform

To send metrics to a monitoring platform, you configure your deployment in two places: create a nozzle that directs the BOSH and CF component metrics from the Firehose, and configure any custom metrics app to send metrics to your monitoring platform directly.

### <a id='nozzle'></a>Create a Nozzle for BOSH and CF Metrics

Metrics originate from the Metron agents on their source components, then travel through Dopplers to the Traffic Controller. The Traffic Controller aggregates the metrics system-wide from the BOSH Health Monitor and CF Components, along with log messages from the same VMs, and emits them all from its Firehose endpoint.

To include BOSH Health Monitor metrics, you must have a BOSH HM Forwarder job running on the VM that runs the BOSH Health Monitor and its Metron agent.

To send BOSH and CF metrics to your logging platform you need to deploy a [nozzle](../loggregator/architecture.html#nozzles) process that takes the Firehose output, ignores the logs, and sends the metrics to your monitoring platform.

See an example nozzle for sending metrics to Datadog [here](https://github.com/cloudfoundry-incubator/datadog-firehose-nozzle). You configure the Datadog account credentials, API location, and other fields and options in the `config/datadog-firehose-nozzle.json` file.

### <a id='custom'></a>Configure Custom Metrics

For production systems, Pivotal recommends deploying an app that runs regular smoke tests and other custom tests and generates metrics from the results.

The app can run in own Docker container, on a [Concourse](http://concourse.ci) VM, or elsewhere.

A custom metrics app sends metrics to the monitoring platform directly, so you need to configure it with your platform endpoint and account information. A custom metrics app does not run a Metron agent, and the Firehose does not carry custom metrics.

See the Pivotal Cloud Ops [CF Smoke Tests](https://github.com/pivotal-cloudops/cf-smoke-tests/tree/Dockerfile) repo for more information and examples of smoke test and custom metrics apps.

See the [Metrics]
(https://concourse.ci/metrics.html) topic in the Concourse documentation for how to set up Concourse to generate custom metrics.

## <a id='config-mon'></a>Configure your Monitoring Platform

Monitoring platforms support two types of monitoring:

* A _dashboard_ for active monitoring when you are at a keyboard and screen
* Automated _alerts_ for when your attention is elsewhere

Some monitoring solutions offer both in one package. Others require putting the two pieces together.

See the [Datadog Config repository](https://github.com/pivotal-cf-experimental/datadog-config-oss/blob/master/cloud-ops-example-doc.md) for an example of how to configure a dashboard and alerts for Cloud Foundry in Datadog.

### <a id='dashboard'></a>Customize Your Dashboard

You customize a dashboard by defining elements on the screen that show values derived from one or more metrics. These dashboard elements typically use simple formulas, such as averaging metric values over the past 60 seconds or summing them up over related instances. They are also often normalized to display with 3 or fewer digits for easy reading and color-coded red, yellow, or green to indicate health conditions.

![Datadog dashboard](./metrics/dashboard.png)

In Datadog, for example, you can define a `screen_template` query that watches the `auctioneer.AuctioneerLRPAuctionsFailed` metric and displays its current average over the past minute.

### <a id='alerts'></a>Create Alerts

You create alerts by defining boolean conditions based on operations over one or more metrics, and an action that the platform takes when an alert triggers. The booleans might check whether metric values exceed or fall below thresholds, for example, or compare metric values against each other.

[In Datadog,](https://github.com/pivotal-cf-experimental/datadog-config-oss/blob/master/alert_templates/shared/diego/LRPs_auction_failure_per_min_too_high.json.erb) you can define an `alert_template` condition that triggers when the `auctioneer.AuctioneerLRPAuctionsFailed` metric indicates an average of more than one failed auction per minute for the past 15 minutes:

<pre><code>
	{
		"query": "min(last_15m):per_minute(avg:datadog.nozzle.auctioneer.AuctioneerLRPAuctionsFailed{deployment:&lt;%= metron_agent_diego_deployment %>}) > 1",
		"message": "##Description:\nDiego internal metrics \n\n## Escalation Path:\nDiego \n\n## Possible Causes:\nThose alerts were a pretty strong signal for us to look at the BBS, which was locked up\n\n## Potential Solutions:\nEscalate to Diego team\n>&lt;%= cloudops_pagerduty %> &lt;%= diego_oncall %>",
		"name": "&lt;%= environment %> Diego: LRP Auction Failure per min is too high",
		"no_data_timeframe": 30,
		"notify_no_data": false
    }
</code></pre>

Actions triggered by an alert can include sending a pager or SMS message, sending an email, generating a support ticket, or passing the alert to a alerting system such as [PagerDuty](https://www.pagerduty.com/).

## <a id='platforms'></a>Monitoring Platforms

Metrics monitoring platforms you can use with Cloud Foundry include:

* [Datadog](https://www.datadoghq.com/)
* [OpenTSDB](http://opentsdb.net/) alerts and [Grafana](http://grafana.org/) dashboard


