---
title: Key Metrics for Monitoring a Pivotal Cloud Foundry Deployment
---

The Pivotal Cloud Ops team monitors the health of its Cloud Foundry deployments using Datadog. [This repository](./https://github.com/pivotal-cf-experimental/datadog-config-oss) contains the customized configuration of Datadog for monitoring production and staging in a CF deployment.

This topic describes the metrics included in the custom-configured Datadog dashboard, and how the Pivotal Cloud Ops team uses them to monitor a PCF deployment.

## <a id='bosh-health'></a>BOSH Health Monitor

<%= image_tag 'metrics/bosh_health_combined.png'%>

### What we monitor 

Health, broken down by component:

- Backend:
	- NATS
	- Doppler
	- Stats
	- HM9000
	- BOSH
	- NAT Box
	- ETCD
- Frontend:
	- Router
	- DEA
	- API
	- UAA

### Why we monitor it

Each row displays the average percentage of "healthy" instances for the relevant component over the last 5 minutes and 24 hours.

For example, suppose your GoRouter has ten instance. If one is not healthy, the stoplight turns red and shows %90.


### The system metric behind it

`bosh.healthmonitor.system.healthy`


### Alerts

!!!TQ: NEED MORE INFO

### Other things to know

Because BOSH restores systems very quickly when they go down, we send alerts generated from this metric to a buffer queue in our alerting system (Pagerduty) that waits two minutes before it sends an alert to the operator. We do this because BOSH/CF restores service reliably enough so that we don't need to be notified in real time for transitory restarts.


## <a id='req-per-sec'></a>Requests per second

<%= image_tag 'metrics/req_per_sec.png'%>

### What we monitor 

Requests per second for each of the following components:
- Router
- DEA
- API
- UAA


### Why we monitor it

To track the basic flow of traffic through the components in the system.

### The system metric behind it

`cf.collector.router.requests(component: app/cloudcontroller/uaa, deployment: production)`

### Alerts

None

### Other things to know

None

## <a id='nats-traffic-delta'></a>NATS Traffic Delta

<%= image_tag 'metrics/nats_delta.png' %>

### What we monitor 

Nats traffic(moving average of 30 minutes) differences over the last hour.

### Why we monitor it

So we can detect significant NATS traffic drop which can indicate a problem with the NATs health.

### The system metric behind it

`aws.ec2.network_in(deployment: production, job:nats_z1/nats_z2}`

### Alerts

None

### Other things to know

None




## <a id='etcd-uptime'></a>ETCD Leader Uptime

<%= image_tag 'metrics/etcd_uptime.png' %>

### What we monitor 

Uptime of the ETCD leader

### Why we monitor it

When the ETCD leader is down this usually either causes or is the symptom of a push failure.

### The system metric behind it

`cloudops_tools.etcd_leader_health`

### Alerts

None

### Other things to know

None



## <a id='ssh-attempts'></a>SSH Attempts

<%= image_tag 'metrics/ssh_attempts.png' %>

### What we monitor 

Count of iptables 'first packet' to port 22 anywhere on the internet.

### Why we monitor it

This is a good indicator of abuse. Malicious users may load an SSH-cracker on to PWS and use it us to launch atacks. We need to know about it and stop them.

### The system metric behind it

`cloudops.tools.ssh-abuse-monitor`

!!!TQ: in one place we have `cloudops_tools...` and in another `cloudops.tools...`; which is it?

### Alerts

None

### Other things to know

"This is a custom CF app that Clould Ops maintains in a private repos (https://github.com/pivotal-cloudops/ssh-abuse-monitor) - itâ€™s not private for any good reason, we just have not released it. The best case is this feature is rolled into the product. The DEAs send their iptables logs to Logsearch, ssh-abuse-monitor polls logsearch and pushes a metric to datadog."

!!!PMQ: erm, what the hell do we do with this?






# <a id='router-status'></a>Router Status Metrics





##  <a id='dea-status'></a> DEA Status




### What we monitor 
### Why we monitor it
### The system metric behind it
### Alerts
### Other things to know
